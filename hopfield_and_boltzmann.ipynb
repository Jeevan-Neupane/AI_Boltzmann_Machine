{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thenaivekid/AI_Boltzmann_Machine/blob/main/hopfield_and_boltzmann.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hopfield Network\n",
        "- Recurrent Network: Every neuron is connected to every other neuron.\n",
        "- Symmetric Weights: The weight matrix\n",
        "ùëä\n",
        "W is symmetric, meaning\n",
        "ùëä\n",
        "ùëñ\n",
        "ùëó\n",
        "=\n",
        "ùëä\n",
        "ùëó\n",
        "ùëñ\n",
        "W\n",
        "ij\n",
        "‚Äã\n",
        " =W\n",
        "ji\n",
        "‚Äã\n",
        " .\n",
        "- Binary or Bipolar Activation: Neurons take values of either {0,1} or {-1,1}.\n",
        "Energy Function: The network updates neuron states to minimize the energy function, making it useful for optimization.\n",
        "- No Self-Connections:\n",
        "ùëä\n",
        "ùëñ\n",
        "ùëñ\n",
        "=\n",
        "0\n",
        "W\n",
        "ii\n",
        "‚Äã\n",
        " =0 (no neuron connects to itself).\n",
        "\n",
        "## üî• Real-World Applications\n",
        "- **Pattern Recognition**: Used in tasks like character recognition and image denoising.\n",
        "- **Content Addressable Memory**: Enables memory recall from partial or noisy inputs.\n",
        "- **Error Correction**: Helps in recovering corrupted or incomplete data.\n",
        "\n",
        "## ‚ö†Ô∏è Weaknesses\n",
        "- **Limited Storage Capacity**: Only about 15% of neurons can store unique patterns.\n",
        "- **Spurious States**: The network may settle into non-stored patterns, leading to incorrect outputs.\n",
        "- **Scalability Issues**: Large Hopfield networks require significant memory and computational time.\n",
        "\n",
        "## Energy Function\n",
        "The energy function of a Hopfield network is given by:\n",
        "\n",
        "\\[\n",
        "E = -\\frac{1}{2} \\sum_{i} \\sum_{j} W_{ij} S_{i} S_{j}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( W_{ij} \\) are the weights between neurons \\( i \\) and \\( j \\).\n",
        "- \\( S_{i} \\) and \\( S_{j} \\) are the states of neurons \\( i \\) and \\( j \\).\n",
        "\n",
        "This energy function helps the network converge to stable states, which correspond to stored patterns."
      ],
      "metadata": {
        "id": "hy-6DZ_R0U8M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 839
        },
        "id": "JxpYtAQzzyxC",
        "outputId": "3d1fbae0-1e77-44a5-c06e-9a8b0d7cc367"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class HopfieldNetwork:\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "        self.weights = np.zeros((size, size))\n",
        "\n",
        "    def activation(self, x):\n",
        "        \"\"\"Activation function using sign function.\"\"\"\n",
        "        return np.where(x >= 0, 1, -1)  # Binary {-1, 1} activation\n",
        "\n",
        "    def train(self, patterns):\n",
        "        \"\"\"Train the Hopfield network using Hebbian learning.\"\"\"\n",
        "        self.weights.fill(0)  # Reset weights\n",
        "        for pattern in patterns:\n",
        "            pattern = np.array(pattern).reshape(self.size, 1)  # Convert to column vector\n",
        "            self.weights += pattern @ pattern.T  # Outer product\n",
        "        np.fill_diagonal(self.weights, 0)  # No self-connections\n",
        "\n",
        "    def recall(self, input_pattern, steps=5):\n",
        "        \"\"\"Recall a pattern from memory using asynchronous updates.\"\"\"\n",
        "        pattern = np.array(input_pattern)\n",
        "        for _ in range(steps):\n",
        "            for i in range(self.size):  # Asynchronous update\n",
        "                net_input = np.dot(self.weights[i], pattern)\n",
        "                pattern[i] = self.activation(net_input)\n",
        "        return pattern\n",
        "\n",
        "    def visualize_pattern(self, pattern, title=\"Pattern\"):\n",
        "        \"\"\"Visualizes a pattern as an image.\"\"\"\n",
        "        dim = int(np.sqrt(self.size))  # Assume square shape\n",
        "        pattern_reshaped = np.array(pattern).reshape(dim, dim)\n",
        "        plt.imshow(pattern_reshaped, cmap='gray')\n",
        "        plt.title(title)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "# Create a large Hopfield network\n",
        "size = 64  # 8x8 grid\n",
        "patterns = [\n",
        "    np.random.choice([-1, 1], size=size) for _ in range(5)  # Store 5 random patterns\n",
        "]\n",
        "\n",
        "hopfield_net = HopfieldNetwork(size)\n",
        "hopfield_net.train(patterns)\n",
        "\n",
        "# Test with a noisy version of a stored pattern\n",
        "test_pattern = patterns[2].copy()\n",
        "test_pattern[:10] *= -1  # Flip first 10 bits (introduce noise)\n",
        "\n",
        "# Visualize input pattern\n",
        "hopfield_net.visualize_pattern(test_pattern, \"Noisy Input\")\n",
        "\n",
        "# Recall from the noisy pattern\n",
        "recalled_pattern = hopfield_net.recall(test_pattern)\n",
        "\n",
        "# Visualize output pattern\n",
        "hopfield_net.visualize_pattern(recalled_pattern, \"Recalled Pattern\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üß† Boltzmann Machine Implementation in Python**\n",
        "The **Boltzmann Machine** is a stochastic neural network that learns by adjusting weights to minimize energy. It differs from the Hopfield network by incorporating **stochastic neuron updates** and using **gradient-based learning**.\n",
        "\n",
        "---\n",
        "\n",
        "### **‚ö° Key Features**\n",
        "‚úÖ Uses **binary stochastic neurons** (sigmoid activation)  \n",
        "‚úÖ **Trains with Gibbs Sampling & Contrastive Divergence (CD-1)**  \n",
        "‚úÖ Supports **hidden and visible units**  \n",
        "‚úÖ Learns patterns from data like **RBMs (Restricted Boltzmann Machines)**  \n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "### **üìù Explanation**\n",
        "1. **Neurons are Stochastic:**  \n",
        "   - Neurons fire based on a probability derived from a **sigmoid function**.\n",
        "2. **Training with Contrastive Divergence (CD-1):**  \n",
        "   - **Positive Phase:** Compute hidden activations based on input.  \n",
        "   - **Negative Phase:** Generate a new visible state using **Gibbs Sampling**.  \n",
        "   - **Update Weights:** Adjust weights using the difference between positive and negative phases.  \n",
        "3. **Inference (Reconstruction):**  \n",
        "   - A test pattern is passed through the network, and Gibbs sampling reconstructs it.\n",
        "\n",
        "---\n",
        "\n",
        "### **üî• Real-World Applications**\n",
        "‚úÖ **Feature Learning** (Used in **RBMs** for deep learning)  \n",
        "‚úÖ **Pattern Completion** (Recovering missing parts of an input)  \n",
        "‚úÖ **Optimization Problems** (Solving combinatorial optimization tasks)  \n",
        "‚úÖ **Cognitive Modeling** (Simulating neural processes in the brain)  \n",
        "\n",
        "---\n",
        "\n",
        "### **‚ö†Ô∏è Weaknesses**\n",
        "‚ùå **Slow Training** (Contrastive Divergence requires iterative sampling)  \n",
        "‚ùå **Difficult Hyperparameter Tuning** (Learning rate, number of hidden units, etc.)  \n",
        "‚ùå **Scaling Issues** (Training large networks is computationally expensive)  \n",
        "\n",
        "---\n",
        "\n",
        "Would you like to integrate it with **real-world datasets** (e.g., MNIST) for better visualization? üöÄ\n"
      ],
      "metadata": {
        "id": "qff2RW5e06uE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class BoltzmannMachine:\n",
        "    def __init__(self, num_visible, num_hidden):\n",
        "        self.num_visible = num_visible\n",
        "        self.num_hidden = num_hidden\n",
        "        self.num_units = num_visible + num_hidden\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights = np.random.normal(0, 0.1, (self.num_units, self.num_units))\n",
        "        np.fill_diagonal(self.weights, 0)  # No self-connections\n",
        "        self.visible_bias = np.zeros(self.num_visible)\n",
        "        self.hidden_bias = np.zeros(self.num_hidden)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"Sigmoid activation function for stochastic updates.\"\"\"\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sample_neurons(self, probabilities):\n",
        "        \"\"\"Sample binary values based on probabilities.\"\"\"\n",
        "        return (np.random.rand(len(probabilities)) < probabilities).astype(int)\n",
        "\n",
        "    def gibbs_sampling(self, visible):\n",
        "        \"\"\"Perform one step of Gibbs sampling.\"\"\"\n",
        "        hidden_prob = self.sigmoid(np.dot(visible, self.weights[:self.num_visible, self.num_visible:]) + self.hidden_bias)\n",
        "        hidden_states = self.sample_neurons(hidden_prob)\n",
        "\n",
        "        visible_prob = self.sigmoid(np.dot(hidden_states, self.weights[self.num_visible:, :self.num_visible]) + self.visible_bias)\n",
        "        visible_states = self.sample_neurons(visible_prob)\n",
        "\n",
        "        return visible_states, hidden_states\n",
        "\n",
        "    def train(self, data, learning_rate=0.1, epochs=1000):\n",
        "        \"\"\"Train using Contrastive Divergence (CD-1).\"\"\"\n",
        "        data = np.array(data)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for sample in data:\n",
        "                # Positive phase (data-driven)\n",
        "                visible_pos = sample\n",
        "                hidden_prob_pos = self.sigmoid(np.dot(visible_pos, self.weights[:self.num_visible, self.num_visible:]) + self.hidden_bias)\n",
        "                hidden_pos = self.sample_neurons(hidden_prob_pos)\n",
        "\n",
        "                # Negative phase (reconstruction via Gibbs sampling)\n",
        "                visible_neg, hidden_neg = self.gibbs_sampling(visible_pos)\n",
        "\n",
        "                # Update weights and biases\n",
        "                self.weights[:self.num_visible, self.num_visible:] += learning_rate * (np.outer(visible_pos, hidden_pos) - np.outer(visible_neg, hidden_neg))\n",
        "                self.weights[self.num_visible:, :self.num_visible] = self.weights[:self.num_visible, self.num_visible:].T  # Symmetric weights\n",
        "\n",
        "                self.visible_bias += learning_rate * (visible_pos - visible_neg)\n",
        "                self.hidden_bias += learning_rate * (hidden_pos - hidden_neg)\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}/{epochs}\")\n",
        "\n",
        "    def reconstruct(self, input_data):\n",
        "        \"\"\"Reconstruct visible data from the hidden layer.\"\"\"\n",
        "        input_data = np.array(input_data)\n",
        "        _, hidden_states = self.gibbs_sampling(input_data)\n",
        "\n",
        "        # Correct shape mismatch: Use hidden states to reconstruct visible units\n",
        "        visible_prob = self.sigmoid(np.dot(hidden_states, self.weights[self.num_visible:, :self.num_visible]) + self.visible_bias)\n",
        "        reconstructed_visible = self.sample_neurons(visible_prob)\n",
        "\n",
        "        return reconstructed_visible\n",
        "\n",
        "# Example Usage\n",
        "num_visible = 8  # Number of visible units\n",
        "num_hidden = 4   # Number of hidden units\n",
        "bm = BoltzmannMachine(num_visible, num_hidden)\n",
        "\n",
        "# Create training data (Binary Patterns)\n",
        "training_data = [\n",
        "    [1, 0, 1, 0, 1, 0, 1, 0],\n",
        "    [0, 1, 0, 1, 0, 1, 0, 1],\n",
        "    [1, 1, 1, 0, 0, 0, 1, 1]\n",
        "]\n",
        "\n",
        "bm.train(training_data, learning_rate=0.1, epochs=1000)\n",
        "\n",
        "# Test with a noisy pattern\n",
        "test_pattern = [1, 0, 1, 0, 0, 0, 1, 0]\n",
        "reconstructed_pattern = bm.reconstruct(test_pattern)\n",
        "print(\"Input Pattern:\", test_pattern)\n",
        "print(\"Reconstructed Pattern:\", reconstructed_pattern)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xv5jvBaB2BUm",
        "outputId": "d6c42b50-e8a4-47d7-bbc5-b257ffcdd1a4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/1000\n",
            "Epoch 100/1000\n",
            "Epoch 200/1000\n",
            "Epoch 300/1000\n",
            "Epoch 400/1000\n",
            "Epoch 500/1000\n",
            "Epoch 600/1000\n",
            "Epoch 700/1000\n",
            "Epoch 800/1000\n",
            "Epoch 900/1000\n",
            "Input Pattern: [1, 0, 1, 0, 0, 0, 1, 0]\n",
            "Reconstructed Pattern: [1 0 1 0 1 0 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_neurons(probabilities):\n",
        "    \"\"\"Sample binary values based on probabilities.\"\"\"\n",
        "    random_probs = np.random.rand(len(probabilities))\n",
        "    print(random_probs)\n",
        "    return (random_probs < probabilities).astype(int)\n",
        "print(sample_neurons([0.5,0.2,0.3]))\n",
        "\n",
        "num_units = 16\n",
        "weights = np.random.normal(0, 0.1, (num_units, num_units))\n",
        "print(weights.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpgLVwHj3Hyl",
        "outputId": "119f9d43-441e-4b53-fc5f-3b4f333cf0c6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.74266595 0.83832965 0.1786093 ]\n",
            "[0 0 1]\n",
            "(16, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Restricted Boltzmann Machine\n",
        "Only connections between visible and hidden units (no intra-layer connections)."
      ],
      "metadata": {
        "id": "J2Kqb4v7By0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Load MNIST Dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: (x > 0.5).float())])  # Binarize images\n",
        "train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# RBM Class\n",
        "class RBM(nn.Module):\n",
        "    def __init__(self, num_visible, num_hidden):\n",
        "        super(RBM, self).__init__()\n",
        "        self.num_visible = num_visible\n",
        "        self.num_hidden = num_hidden\n",
        "        self.W = nn.Parameter(torch.randn(num_hidden, num_visible) * 0.01)  # Weights\n",
        "        self.v_bias = nn.Parameter(torch.zeros(num_visible))  # Visible bias\n",
        "        self.h_bias = nn.Parameter(torch.zeros(num_hidden))  # Hidden bias\n",
        "\n",
        "    def forward(self, v):\n",
        "        \"\"\"One Gibbs sampling step: v -> h -> v'\"\"\"\n",
        "        h_prob = torch.sigmoid(torch.matmul(v, self.W.T) + self.h_bias)  # P(h|v)\n",
        "        h_state = (torch.rand_like(h_prob) < h_prob).float()  # Sample h\n",
        "        v_prob = torch.sigmoid(torch.matmul(h_state, self.W) + self.v_bias)  # P(v|h)\n",
        "        v_state = (torch.rand_like(v_prob) < v_prob).float()  # Sample v\n",
        "        return v_prob, v_state\n",
        "\n",
        "    def free_energy(self, v):\n",
        "        \"\"\"Energy function for Contrastive Divergence.\"\"\"\n",
        "        # Term 1: Visible bias term (v^T * b_v)\n",
        "        vb_term = torch.matmul(v, self.v_bias)\n",
        "\n",
        "        # Term 2: Hidden term (sum over log(1 + exp(v^T * W_j + b_h_j)))\n",
        "        hidden_term = torch.sum(\n",
        "            torch.log(1 + torch.exp(torch.matmul(v, self.W.T) + self.h_bias)),\n",
        "            dim=1\n",
        "        )\n",
        "\n",
        "        # Free energy: F(v) = -vb_term - hidden_term\n",
        "        return -vb_term - hidden_term\n",
        "\n",
        "    def train_rbm(self, train_loader, lr=0.001, epochs=50):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=lr)  # Use Adam instead of SGD\n",
        "        loss_history = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0\n",
        "            for batch, (data, _) in enumerate(train_loader):\n",
        "                v0 = data.view(-1, 28*28).to(device)  # Flatten images & move to GPU\n",
        "                v1_prob, v1_state = self.forward(v0)  # Gibbs sampling\n",
        "\n",
        "                # Compute gradients using Contrastive Divergence (CD-1)\n",
        "                loss = torch.mean(self.free_energy(v0)) - torch.mean(self.free_energy(v1_state))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            avg_loss = epoch_loss / len(train_loader)\n",
        "            loss_history.append(avg_loss)\n",
        "            print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "            # Visualize Weights & Reconstructions\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                self.visualize_weights()\n",
        "                self.visualize_reconstruction(v0, v1_prob)\n",
        "\n",
        "        # Plot loss curve\n",
        "        self.plot_loss(loss_history)\n",
        "\n",
        "    def plot_loss(self, loss_history):\n",
        "        \"\"\"Plot training loss curve.\"\"\"\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.plot(loss_history, label=\"Loss\")\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"Free Energy Loss\")\n",
        "        plt.title(\"RBM Training Loss Curve\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    def visualize_weights(self, num_images=16):\n",
        "        \"\"\"Plot learned features (weights).\"\"\"\n",
        "        fig, axes = plt.subplots(4, 4, figsize=(6, 6))\n",
        "        for i, ax in enumerate(axes.flatten()):\n",
        "            if i >= num_images:\n",
        "                break\n",
        "            weight_img = self.W[i].detach().cpu().view(28, 28)\n",
        "            ax.imshow(weight_img, cmap=\"gray\")\n",
        "            ax.axis(\"off\")\n",
        "        plt.suptitle(\"RBM Learned Features\")\n",
        "        plt.show()\n",
        "\n",
        "    def visualize_reconstruction(self, original, reconstructed, num_images=10):\n",
        "        \"\"\"Visualize original and reconstructed images.\"\"\"\n",
        "        fig, axes = plt.subplots(2, num_images, figsize=(15, 3))\n",
        "        for i in range(num_images):\n",
        "            # Original\n",
        "            axes[0, i].imshow(original[i].detach().cpu().view(28, 28), cmap=\"gray\")\n",
        "            axes[0, i].axis(\"off\")\n",
        "\n",
        "            # Reconstructed\n",
        "            axes[1, i].imshow(reconstructed[i].detach().cpu().view(28, 28), cmap=\"gray\")\n",
        "            axes[1, i].axis(\"off\")\n",
        "\n",
        "        axes[0, 0].set_title(\"Original Images\")\n",
        "        axes[1, 0].set_title(\"Reconstructed Images\")\n",
        "        plt.show()\n",
        "\n",
        "    def evaluate(self, test_loader):\n",
        "        \"\"\"Evaluate RBM on unseen test data using MSE, SSIM, and PSNR.\"\"\"\n",
        "        mse_total, ssim_total, psnr_total, count = 0, 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for batch, (data, _) in enumerate(test_loader):\n",
        "                v0 = data.view(-1, 28*28).to(device)  # Flatten images & move to GPU\n",
        "                v1_prob, _ = self.forward(v0)  # Reconstruct images\n",
        "\n",
        "                # Convert to numpy for metric calculations\n",
        "                original_np = v0.cpu().numpy()\n",
        "                reconstructed_np = v1_prob.cpu().numpy()\n",
        "\n",
        "                # Compute MSE, SSIM, PSNR\n",
        "                for i in range(original_np.shape[0]):\n",
        "                    mse = np.mean((original_np[i] - reconstructed_np[i])**2)\n",
        "                    ssim_score = ssim(original_np[i].reshape(28, 28), reconstructed_np[i].reshape(28, 28), data_range=1)\n",
        "                    psnr_score = psnr(original_np[i], reconstructed_np[i], data_range=1)\n",
        "\n",
        "                    mse_total += mse\n",
        "                    ssim_total += ssim_score\n",
        "                    psnr_total += psnr_score\n",
        "                    count += 1\n",
        "\n",
        "\n",
        "\n",
        "        # Print results\n",
        "        print(\"\\n--- RBM Evaluation on Unseen Data ---\")\n",
        "        print(f\"Mean Squared Error (MSE): {mse_total / count:.5f}\")\n",
        "        print(f\"Structural Similarity Index (SSIM): {ssim_total / count:.5f}\")\n",
        "        print(f\"Peak Signal-to-Noise Ratio (PSNR): {psnr_total / count:.5f}\")\n",
        "\n",
        "        # Visualize some reconstructions\n",
        "        self.visualize_reconstruction(v0, v1_prob)\n",
        "\n",
        "    def extract_features(self, data_loader):\n",
        "        \"\"\"Extract hidden layer features for classification\"\"\"\n",
        "        features, labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch, (data, targets) in enumerate(data_loader):\n",
        "                v = data.view(-1, 28 * 28).to(device)\n",
        "                h_prob, _ = self.forward(v)  # Get hidden layer activations\n",
        "                features.append(h_prob.cpu().numpy())\n",
        "                labels.append(targets.cpu().numpy())\n",
        "\n",
        "        return np.vstack(features), np.hstack(labels)\n",
        "\n",
        "# # Train RBM\n",
        "rbm = RBM(num_visible=28*28, num_hidden=256).to(device)  # 256 hidden neurons, move to GPU\n",
        "# rbm.train_rbm(train_loader, lr=0.001, epochs=50)\n",
        "\n",
        "# # Evaluate on Unseen Test Data\n",
        "# rbm.evaluate(test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R61DBUnU5wSk",
        "outputId": "f2dae35e-a685-49c9-a992-f931084780ee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.91M/9.91M [00:01<00:00, 5.34MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28.9k/28.9k [00:00<00:00, 159kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.65M/1.65M [00:01<00:00, 1.30MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.54k/4.54k [00:00<00:00, 5.98MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# torch.save(rbm.state_dict(), \"/content/drive/MyDrive/research/rbm_mnist.pth\")\n",
        "# print(\"‚úÖ Model saved as rbm_mnist.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVd6gtvS_gJe",
        "outputId": "7158685c-6081-4df0-ff45-86faa16c9b6e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "rbm_loaded = RBM(num_visible=28*28, num_hidden=256).to(device)  # Initialize same architecture\n",
        "rbm_loaded.load_state_dict(torch.load(\"/content/drive/MyDrive/research/rbm_mnist.pth\", map_location=torch.device('cpu')))\n",
        "rbm_loaded.eval()  # Set to evaluation mode\n",
        "print(\"‚úÖ Model loaded successfully\")\n",
        "rbm = rbm_loaded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABcu_3enAD_A",
        "outputId": "33e358e8-f106-4aa7-9dfb-67bd99d8ded4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-88555daaefe1>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  rbm_loaded.load_state_dict(torch.load(\"/content/drive/MyDrive/research/rbm_mnist.pth\", map_location=torch.device('cpu')))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model loaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import torchvision\n",
        "# import torchvision.transforms as transforms\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "# train_features, train_labels = rbm.extract_features(train_loader)\n",
        "# test_features, test_labels = rbm.extract_features(test_loader)\n",
        "\n",
        "# # Train Classifier (Logistic Regression)\n",
        "# classifier = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='multinomial')\n",
        "# classifier.fit(train_features, train_labels)\n",
        "\n",
        "# # Evaluate Model\n",
        "# predictions = classifier.predict(test_features)\n",
        "# accuracy = accuracy_score(test_labels, predictions)\n",
        "# print(f\"Classification Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "9F6KyGANbCS3",
        "outputId": "8afa78cd-a623-4ffa-f84b-0a05d69538d0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e491f1416559>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Train Classifier (Logistic Regression)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'multinomial'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Evaluate Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0mn_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n\u001b[0m\u001b[1;32m   1351\u001b[0m             path_func(\n\u001b[1;32m   1352\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    449\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             ]\n\u001b[0;32m--> 451\u001b[0;31m             opt_res = optimize.minimize(\n\u001b[0m\u001b[1;32m    452\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    711\u001b[0m                                  **options)\n\u001b[1;32m    712\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m         res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[1;32m    714\u001b[0m                                callback=callback, **options)\n\u001b[1;32m    715\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/optimize/_lbfgsb_py.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;31m# Overwriting results in undefined behaviour because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;31m# fun(self.x) will change self.x, with the two no longer linked.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;31m# Make sure the function returns a true scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;34m\"\"\" returns the function value \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m_compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_linear_loss.py\u001b[0m in \u001b[0;36mloss_gradient\u001b[0;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_intercept\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         loss, grad_pointwise = self.base_loss.loss_gradient(\n\u001b[0m\u001b[1;32m    317\u001b[0m             \u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mraw_prediction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw_prediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/_loss/loss.py\u001b[0m in \u001b[0;36mloss_gradient\u001b[0;34m(self, y_true, raw_prediction, sample_weight, loss_out, gradient_out, n_threads)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     def loss_gradient(\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import joblib\n",
        "# joblib.dump(classifier, \"/content/drive/MyDrive/research/logistic_classifier.pkl\")\n",
        "\n",
        "# print(\"Models saved successfully!\")\n"
      ],
      "metadata": {
        "id": "bSipUgXcb4eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Extract features using the trained RBM\n",
        "train_features, train_labels = rbm.extract_features(train_loader)\n",
        "test_features, test_labels = rbm.extract_features(test_loader)\n",
        "\n",
        "# Train SVM Classifier\n",
        "# classifier = SVC(kernel='rbf', C=1.0, gamma='scale')  # RBF kernel for better performance\n",
        "# classifier.fit(train_features, train_labels)\n",
        "\n",
        "# # Evaluate Model\n",
        "# predictions = classifier.predict(test_features)\n",
        "# accuracy = accuracy_score(test_labels, predictions)\n",
        "# print(f\"Classification Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "NM9wB2_ZcS72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import joblib\n",
        "# joblib.dump(classifier, \"/content/drive/MyDrive/research/svm_classifier.pkl\")\n",
        "\n",
        "# print(\"Models saved successfully!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "mAoxkwNacU_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import joblib\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "# Predict using the loaded classifier\n",
        "# Load trained SVM classifier\n",
        "svm_path = \"/content/drive/MyDrive/research/svm_classifier.pkl\"\n",
        "classifier = joblib.load(svm_path)\n",
        "\n",
        "predictions = classifier.predict(test_features)\n",
        "\n",
        "# Classification Report (Precision, Recall, F1-Score, Support)\n",
        "report = classification_report(test_labels, predictions)\n",
        "print(\"Classification Report:\\n\", report)\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(test_labels, predictions)\n",
        "\n",
        "# Plot confusion matrix using seaborn heatmap for better visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 0\", \"Class 1\", \"Class 2\"], yticklabels=[\"Class 0\", \"Class 1\", \"Class 2\"])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Overall accuracy\n",
        "accuracy = accuracy_score(test_labels, predictions)\n",
        "print(f\"Overall Classification Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "THrjOBvtpVLF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 930
        },
        "outputId": "7c405526-627d-4cd1-ac35-fb8de00239a6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98       980\n",
            "           1       0.99      0.99      0.99      1135\n",
            "           2       0.97      0.97      0.97      1032\n",
            "           3       0.97      0.98      0.97      1010\n",
            "           4       0.97      0.96      0.97       982\n",
            "           5       0.97      0.97      0.97       892\n",
            "           6       0.98      0.98      0.98       958\n",
            "           7       0.97      0.96      0.97      1028\n",
            "           8       0.97      0.97      0.97       974\n",
            "           9       0.97      0.95      0.96      1009\n",
            "\n",
            "    accuracy                           0.97     10000\n",
            "   macro avg       0.97      0.97      0.97     10000\n",
            "weighted avg       0.97      0.97      0.97     10000\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Classification Accuracy: 0.9733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gibbs_sampling(rbm, num_samples=10, num_iterations=10):\n",
        "    \"\"\"Generate new samples by alternating Gibbs sampling between visible and hidden units.\"\"\"\n",
        "    # Randomly initialize visible states (MNIST images are binary: 0 or 1)\n",
        "    v = torch.rand(num_samples, rbm.num_visible, device=device) > 0.5  # Random binary visible state\n",
        "    v = v.float()  # Convert to float for matrix operations\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        # Step 1: Sample hidden states from visible states\n",
        "        h_prob = torch.sigmoid(torch.matmul(v, rbm.W.t()) + rbm.h_bias)  # Compute hidden probabilities\n",
        "        h = (h_prob > 0.5).float()  # Sample hidden states\n",
        "\n",
        "        # Step 2: Sample visible states from hidden states\n",
        "        v_prob = torch.sigmoid(torch.matmul(h, rbm.W) + rbm.v_bias)  # Compute visible probabilities\n",
        "        v = (v_prob > 0.5).float()  # Sample visible states\n",
        "\n",
        "    # Visualize generated images\n",
        "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n",
        "    for i in range(num_samples):\n",
        "        axes[i].imshow(v[i].cpu().view(28, 28), cmap=\"gray\")\n",
        "        axes[i].axis(\"off\")\n",
        "    plt.suptitle(\"Generated Images from RBM\")\n",
        "    plt.show()\n",
        "\n",
        "# Generate new images using Gibbs sampling\n",
        "gibbs_sampling(rbm_loaded, num_samples=10, num_iterations=20)\n"
      ],
      "metadata": {
        "id": "pSv2-q75AWHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pdz8Z6gca4nL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **What is a Deep Belief Network (DBN)?**\n",
        "A **Deep Belief Network (DBN)** is a generative model made up of multiple layers of **Restricted Boltzmann Machines (RBMs)** stacked on top of each other. DBNs are used for unsupervised learning and are good at learning hierarchical feature representations of the input data.\n",
        "\n",
        "The basic structure of a DBN is:\n",
        "1. **Input Layer**: The first layer that takes in raw data (e.g., images, text, etc.).\n",
        "2. **Hidden Layers**: Multiple hidden layers (each one being an RBM) that progressively learn more abstract features.\n",
        "3. **Output Layer**: The final layer typically used for classification or reconstruction tasks.\n",
        "\n",
        "### **How does a DBN work?**\n",
        "1. **Layer-Wise Pre-Training (Unsupervised Learning)**: Each layer of the DBN is trained as a **Restricted Boltzmann Machine (RBM)**. In this phase, the model learns to reconstruct the input data by capturing the dependencies in the data.\n",
        "   - The bottom-up phase involves passing the input data through the network to learn the parameters.\n",
        "   - The top-down phase involves reconstructing the data from the learned features.\n",
        "   \n",
        "2. **Fine-Tuning (Supervised Learning)**: After pre-training, the whole DBN is fine-tuned using supervised learning techniques (like backpropagation) to improve the network for tasks like classification.\n",
        "\n",
        "### **Steps to Create a DBN:**\n",
        "1. **Pre-training with RBMs**: Pre-train each layer of the DBN as an RBM.\n",
        "2. **Fine-Tuning**: Once all layers are pre-trained, fine-tune the entire network using a supervised learning algorithm.\n",
        "\n",
        "### **DBN Components:**\n",
        "1. **RBM**: A neural network layer used in DBN, which is essentially a bipartite graph where each visible unit is connected to every hidden unit.\n",
        "2. **Contrastive Divergence (CD)**: The algorithm used to train each RBM. It updates the weights based on the difference between the data-driven and model-driven distributions.\n",
        "\n",
        "### **Training a DBN:**\n",
        "1. **Layer-Wise Pre-Training**: Train each RBM one by one on the data.\n",
        "2. **Fine-Tuning**: Use gradient-based optimization methods (like SGD or Adam) to fine-tune the entire DBN model.\n",
        "\n",
        "---\n",
        "\n",
        "### **Implementation of DBN with PyTorch:**\n",
        "\n",
        "Let's break down the process of implementing a **Deep Belief Network** with PyTorch.\n",
        "\n",
        "1. **Define the RBM Layer**: We'll define a basic RBM layer first. An RBM consists of visible and hidden layers, and we need to learn the weights that connect these layers.\n",
        "\n",
        "2. **Pre-training the DBN**: We'll use Contrastive Divergence (CD) to train the RBM layers one by one.\n",
        "\n",
        "3. **Fine-Tuning the DBN**: After pre-training, we will fine-tune the whole network using backpropagation.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Summary of Steps:**\n",
        "\n",
        "1. **Define the RBM layer** and train it using contrastive divergence.\n",
        "2. **Pre-train the DBN** layer by layer using RBMs.\n",
        "3. **Fine-tune the DBN** with a classifier head using backpropagation.\n",
        "4. **Generate samples** from the trained DBN.\n",
        "\n",
        "### **Next Steps:**\n",
        "- After training the model, you can use it for classification tasks or generate new data by sampling from the visible units after pre-training.\n",
        "- You can also explore various optimizers and experiment with hyperparameters.\n",
        "\n",
        "This will give you an introduction to **Deep Belief Networks (DBN)**. Let me know if you'd like me to dive deeper into any part!# Deep Belief Network"
      ],
      "metadata": {
        "id": "da2lROOm3IGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "class RBM(nn.Module):\n",
        "    \"\"\"Restricted Boltzmann Machine implementation\"\"\"\n",
        "\n",
        "    def __init__(self, n_visible, n_hidden):\n",
        "        super(RBM, self).__init__()\n",
        "        self.n_visible = n_visible\n",
        "        self.n_hidden = n_hidden\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights = nn.Parameter(torch.randn(n_visible, n_hidden) * 0.01)\n",
        "        self.v_bias = nn.Parameter(torch.zeros(n_visible))\n",
        "        self.h_bias = nn.Parameter(torch.zeros(n_hidden))\n",
        "\n",
        "    def visible_to_hidden(self, v):\n",
        "        \"\"\"Compute hidden probabilities given visible units\"\"\"\n",
        "        hidden_activation = F.linear(v, self.weights.t(), self.h_bias)\n",
        "        hidden_probs = torch.sigmoid(hidden_activation)\n",
        "        return hidden_probs\n",
        "\n",
        "    def hidden_to_visible(self, h):\n",
        "        \"\"\"Compute visible probabilities given hidden units\"\"\"\n",
        "        visible_activation = F.linear(h, self.weights, self.v_bias)\n",
        "        visible_probs = torch.sigmoid(visible_activation)\n",
        "        return visible_probs\n",
        "\n",
        "    def sample_hidden(self, v):\n",
        "        \"\"\"Sample hidden states given visible states\"\"\"\n",
        "        hidden_probs = self.visible_to_hidden(v)\n",
        "        hidden_states = torch.bernoulli(hidden_probs)\n",
        "        return hidden_states, hidden_probs\n",
        "\n",
        "    def sample_visible(self, h):\n",
        "        \"\"\"Sample visible states given hidden states\"\"\"\n",
        "        visible_probs = self.hidden_to_visible(h)\n",
        "        visible_states = torch.bernoulli(visible_probs)\n",
        "        return visible_states, visible_probs\n",
        "\n",
        "    def free_energy(self, v):\n",
        "        \"\"\"Compute free energy for a visible vector\"\"\"\n",
        "        hidden_activation = F.linear(v, self.weights.t(), self.h_bias)\n",
        "        visible_bias_term = torch.matmul(v, self.v_bias)\n",
        "        hidden_term = torch.sum(F.softplus(hidden_activation), dim=1)\n",
        "        return -visible_bias_term - hidden_term\n",
        "\n",
        "    def forward(self, v):\n",
        "        \"\"\"Forward pass (for compatibility with PyTorch modules)\"\"\"\n",
        "        return self.visible_to_hidden(v)\n",
        "\n",
        "    def contrastive_divergence(self, v_data, k=1):\n",
        "        \"\"\"Perform k-step contrastive divergence\"\"\"\n",
        "        # Positive phase\n",
        "        h_data, h_data_prob = self.sample_hidden(v_data)\n",
        "\n",
        "        # Negative phase (Gibbs sampling)\n",
        "        v_model = v_data.clone()\n",
        "        h_model = h_data.clone()\n",
        "\n",
        "        for _ in range(k):\n",
        "            v_model, v_model_prob = self.sample_visible(h_model)\n",
        "            h_model, h_model_prob = self.sample_hidden(v_model)\n",
        "\n",
        "        # Compute gradients\n",
        "        positive_grad = torch.matmul(v_data.t(), h_data_prob)\n",
        "        negative_grad = torch.matmul(v_model.t(), h_model_prob)\n",
        "\n",
        "        # Return gradients for weight, visible bias, and hidden bias\n",
        "        return {\n",
        "            'weights': (positive_grad - negative_grad) / v_data.size(0),\n",
        "            'v_bias': torch.mean(v_data - v_model, dim=0),\n",
        "            'h_bias': torch.mean(h_data_prob - h_model_prob, dim=0)\n",
        "        }\n",
        "\n",
        "def train_rbm(rbm, dataloader, epochs=10, lr=0.01, k=1, device='cpu'):\n",
        "    \"\"\"Train a single RBM layer\"\"\"\n",
        "    optimizer = optim.SGD(rbm.parameters(), lr=lr)\n",
        "\n",
        "    rbm.to(device)\n",
        "    reconstruction_errors = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_error = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for batch_idx, (data, _) in enumerate(dataloader):\n",
        "            data = data.view(data.size(0), -1).to(device)  # Flatten the data\n",
        "\n",
        "            # Normalize data to [0,1] if needed\n",
        "            if data.max() > 1:\n",
        "                data = data / 255.0\n",
        "\n",
        "            # Apply bernoulli sampling if data is real-valued\n",
        "            v_data = torch.bernoulli(data)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Compute gradients using contrastive divergence\n",
        "            grads = rbm.contrastive_divergence(v_data, k)\n",
        "\n",
        "            # Manually set gradients\n",
        "            rbm.weights.grad = -grads['weights']\n",
        "            rbm.v_bias.grad = -grads['v_bias']\n",
        "            rbm.h_bias.grad = -grads['h_bias']\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # Compute reconstruction error\n",
        "            with torch.no_grad():\n",
        "                h, _ = rbm.sample_hidden(v_data)\n",
        "                v_recon, _ = rbm.sample_visible(h)\n",
        "                batch_error = F.mse_loss(v_data, v_recon)\n",
        "                epoch_error += batch_error.item()\n",
        "\n",
        "            if (batch_idx + 1) % 100 == 0:\n",
        "                print(f'Epoch: {epoch+1}/{epochs} [{batch_idx+1}/{len(dataloader)}] Reconstruction error: {batch_error.item():.4f}')\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        avg_epoch_error = epoch_error / len(dataloader)\n",
        "        reconstruction_errors.append(avg_epoch_error)\n",
        "        print(f'Epoch: {epoch+1}/{epochs} completed in {elapsed:.2f}s. Average reconstruction error: {avg_epoch_error:.4f}')\n",
        "\n",
        "    return reconstruction_errors\n",
        "\n",
        "class DBN(nn.Module):\n",
        "    \"\"\"Deep Belief Network implementation\"\"\"\n",
        "\n",
        "    def __init__(self, visible_size, hidden_sizes):\n",
        "        \"\"\"\n",
        "        Initialize a Deep Belief Network\n",
        "\n",
        "        Args:\n",
        "            visible_size: Size of the input layer\n",
        "            hidden_sizes: List of sizes for the hidden layers\n",
        "        \"\"\"\n",
        "        super(DBN, self).__init__()\n",
        "\n",
        "        self.n_layers = len(hidden_sizes)\n",
        "        layer_sizes = [visible_size] + hidden_sizes\n",
        "\n",
        "        # Create RBM layers\n",
        "        self.rbm_layers = nn.ModuleList([\n",
        "            RBM(layer_sizes[i], layer_sizes[i+1])\n",
        "            for i in range(self.n_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through all RBM layers\"\"\"\n",
        "        for i, rbm in enumerate(self.rbm_layers):\n",
        "            x, _ = rbm.sample_hidden(x)\n",
        "        return x\n",
        "\n",
        "    def pretrain(self, dataloader, epochs_per_layer=10, lr=0.01, cd_k=1, device='cpu'):\n",
        "        \"\"\"\n",
        "        Greedy layer-wise pretraining of the DBN\n",
        "\n",
        "        Args:\n",
        "            dataloader: DataLoader for training data\n",
        "            epochs_per_layer: Number of epochs to train each layer\n",
        "            lr: Learning rate\n",
        "            cd_k: Number of contrastive divergence steps\n",
        "            device: Device to run training on\n",
        "        \"\"\"\n",
        "        self.to(device)\n",
        "\n",
        "        # Train each RBM layer\n",
        "        for i, rbm in enumerate(self.rbm_layers):\n",
        "            print(f\"\\n{'='*50}\")\n",
        "            print(f\"Pretraining RBM layer {i+1}/{self.n_layers}\")\n",
        "            print(f\"{'='*50}\")\n",
        "\n",
        "            # For first layer, use original data\n",
        "            if i == 0:\n",
        "                train_rbm(rbm, dataloader, epochs=epochs_per_layer, lr=lr, k=cd_k, device=device)\n",
        "\n",
        "            # For subsequent layers, transform data using previous layers\n",
        "            else:\n",
        "                print(f\"Transforming data for layer {i+1}...\")\n",
        "                transformed_data = []\n",
        "                transformed_labels = []\n",
        "\n",
        "                # Use previous layers to transform the input data\n",
        "                with torch.no_grad():\n",
        "                    for batch, labels in dataloader:\n",
        "                        batch = batch.view(batch.size(0), -1).to(device)\n",
        "                        if batch.max() > 1:\n",
        "                            batch = batch / 255.0\n",
        "\n",
        "                        # Pass data through previous layers\n",
        "                        for prev_rbm in self.rbm_layers[:i]:\n",
        "                            batch, _ = prev_rbm.sample_hidden(batch)\n",
        "\n",
        "                        transformed_data.append(batch)\n",
        "                        transformed_labels.append(labels)\n",
        "\n",
        "                # Create a new dataloader with transformed data\n",
        "                transformed_data = torch.cat(transformed_data, 0)\n",
        "                transformed_labels = torch.cat(transformed_labels, 0)\n",
        "                transformed_dataset = TensorDataset(transformed_data, transformed_labels)\n",
        "                transformed_dataloader = DataLoader(\n",
        "                    transformed_dataset,\n",
        "                    batch_size=dataloader.batch_size,\n",
        "                    shuffle=True\n",
        "                )\n",
        "\n",
        "                # Train current RBM with transformed data\n",
        "                train_rbm(rbm, transformed_dataloader, epochs=epochs_per_layer, lr=lr, k=cd_k, device=device)\n",
        "\n",
        "class DBNClassifier(nn.Module):\n",
        "    \"\"\"DBN with a classifier layer for supervised learning\"\"\"\n",
        "\n",
        "    def __init__(self, visible_size, hidden_sizes, output_size):\n",
        "        super(DBNClassifier, self).__init__()\n",
        "\n",
        "        # DBN for feature extraction\n",
        "        self.dbn = DBN(visible_size, hidden_sizes)\n",
        "\n",
        "        # Classifier layer\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_sizes[-1], output_size),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Feature extraction\n",
        "        features = self.dbn(x)\n",
        "\n",
        "        # Classification\n",
        "        return self.classifier(features)\n",
        "\n",
        "    def pretrain(self, dataloader, epochs_per_layer=10, lr=0.01, cd_k=1, device='cpu'):\n",
        "        \"\"\"Pretrain the DBN layers\"\"\"\n",
        "        return self.dbn.pretrain(dataloader, epochs_per_layer, lr, cd_k, device)\n",
        "\n",
        "    def finetune(self, train_dataloader, test_dataloader=None, epochs=10, lr=0.01, device='cpu'):\n",
        "        \"\"\"Fine-tune the entire network using backpropagation\"\"\"\n",
        "        self.to(device)\n",
        "\n",
        "        criterion = nn.NLLLoss()\n",
        "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
        "\n",
        "        train_losses = []\n",
        "        test_losses = []\n",
        "        train_accuracies = []\n",
        "        test_accuracies = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training\n",
        "            self.train()\n",
        "            running_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "            for data, targets in train_dataloader:\n",
        "                data = data.view(data.size(0), -1).to(device)\n",
        "                if data.max() > 1:\n",
        "                    data = data / 255.0\n",
        "                targets = targets.to(device)\n",
        "\n",
        "                # Zero gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self(data)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                # Backward pass and optimization\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Statistics\n",
        "                running_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += targets.size(0)\n",
        "                correct += (predicted == targets).sum().item()\n",
        "\n",
        "            train_loss = running_loss / len(train_dataloader)\n",
        "            train_accuracy = 100 * correct / total\n",
        "            train_losses.append(train_loss)\n",
        "            train_accuracies.append(train_accuracy)\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f'Epoch: {epoch+1}/{epochs} completed in {elapsed:.2f}s')\n",
        "            print(f'Training Loss: {train_loss:.4f} Accuracy: {train_accuracy:.2f}%')\n",
        "\n",
        "            # Testing if test_dataloader is provided\n",
        "            if test_dataloader is not None:\n",
        "                self.eval()\n",
        "                test_loss = 0.0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    for data, targets in test_dataloader:\n",
        "                        data = data.view(data.size(0), -1).to(device)\n",
        "                        if data.max() > 1:\n",
        "                            data = data / 255.0\n",
        "                        targets = targets.to(device)\n",
        "\n",
        "                        # Forward pass\n",
        "                        outputs = self(data)\n",
        "\n",
        "                        # Compute loss\n",
        "                        loss = criterion(outputs, targets)\n",
        "                        test_loss += loss.item()\n",
        "\n",
        "                        # Statistics\n",
        "                        _, predicted = torch.max(outputs.data, 1)\n",
        "                        total += targets.size(0)\n",
        "                        correct += (predicted == targets).sum().item()\n",
        "\n",
        "                test_loss = test_loss / len(test_dataloader)\n",
        "                test_accuracy = 100 * correct / total\n",
        "                test_losses.append(test_loss)\n",
        "                test_accuracies.append(test_accuracy)\n",
        "\n",
        "                print(f'Test Loss: {test_loss:.4f} Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "                # Update learning rate\n",
        "                scheduler.step(test_loss)\n",
        "\n",
        "            print('-' * 50)\n",
        "\n",
        "        # Return training history\n",
        "        history = {\n",
        "            'train_loss': train_losses,\n",
        "            'train_accuracy': train_accuracies,\n",
        "            'test_loss': test_losses,\n",
        "            'test_accuracy': test_accuracies\n",
        "        }\n",
        "\n",
        "        return history\n",
        "\n",
        "def visualize_weights(rbm, nrow=10, ncol=10, figsize=(10, 10)):\n",
        "    \"\"\"Visualize the weights of an RBM as images\"\"\"\n",
        "    weights = rbm.weights.data.cpu().numpy()\n",
        "\n",
        "    # Determine the image shape based on the visible units\n",
        "    if rbm.n_visible in [784, 28*28]:  # MNIST\n",
        "        img_shape = (28, 28)\n",
        "    else:\n",
        "        # Try to find factors for a square-ish image\n",
        "        side = int(np.sqrt(rbm.n_visible))\n",
        "        img_shape = (side, rbm.n_visible // side)\n",
        "\n",
        "    fig, axes = plt.subplots(nrow, ncol, figsize=figsize)\n",
        "\n",
        "    # Select a subset of hidden units to visualize\n",
        "    n_vis = min(nrow * ncol, rbm.n_hidden)\n",
        "\n",
        "    for i in range(nrow):\n",
        "        for j in range(ncol):\n",
        "            idx = i * ncol + j\n",
        "            if idx < n_vis:\n",
        "                # Reshape weights for this hidden unit into an image\n",
        "                weight_img = weights[:, idx].reshape(img_shape)\n",
        "\n",
        "                # Display the image\n",
        "                axes[i, j].imshow(weight_img, cmap='gray')\n",
        "                axes[i, j].axis('off')\n",
        "            else:\n",
        "                axes[i, j].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('rbm_weights.png')\n",
        "    plt.show()\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"Plot training and test loss/accuracy\"\"\"\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['train_loss'], label='Train Loss')\n",
        "    if history['test_loss']:\n",
        "        plt.plot(history['test_loss'], label='Test Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Loss during Training')\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['train_accuracy'], label='Train Accuracy')\n",
        "    if history['test_accuracy']:\n",
        "        plt.plot(history['test_accuracy'], label='Test Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.title('Accuracy during Training')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_history.png')\n",
        "    plt.show()\n",
        "\n",
        "def generate_samples(rbm, num_samples=10, gibbs_steps=1000, device='cpu'):\n",
        "    \"\"\"Generate samples from an RBM using Gibbs sampling\"\"\"\n",
        "    rbm.to(device)\n",
        "\n",
        "    # Start with random visible units\n",
        "    v = torch.rand(num_samples, rbm.n_visible).to(device)\n",
        "\n",
        "    # Perform Gibbs sampling\n",
        "    for _ in range(gibbs_steps):\n",
        "        h, _ = rbm.sample_hidden(v)\n",
        "        v, _ = rbm.sample_visible(h)\n",
        "\n",
        "    # Visualize the generated samples\n",
        "    plt.figure(figsize=(15, 3))\n",
        "\n",
        "    # Determine the image shape based on the visible units\n",
        "    if rbm.n_visible in [784, 28*28]:  # MNIST\n",
        "        img_shape = (28, 28)\n",
        "    else:\n",
        "        # Try to find factors for a square-ish image\n",
        "        side = int(np.sqrt(rbm.n_visible))\n",
        "        img_shape = (side, rbm.n_visible // side)\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        plt.subplot(1, num_samples, i+1)\n",
        "        plt.imshow(v[i].cpu().detach().view(img_shape), cmap='gray')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('generated_samples.png')\n",
        "    plt.show()\n",
        "\n",
        "    return v\n",
        "\n",
        "def load_mnist(batch_size=64):\n",
        "    \"\"\"Load MNIST dataset and create train/test dataloaders\"\"\"\n",
        "    # Define transformations\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    # Download and load MNIST dataset\n",
        "    train_dataset = torchvision.datasets.MNIST(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        transform=transform,\n",
        "        download=True\n",
        "    )\n",
        "\n",
        "    test_dataset = torchvision.datasets.MNIST(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        transform=transform,\n",
        "        download=True\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "def main():\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(42)\n",
        "\n",
        "    # Load MNIST dataset\n",
        "    print(\"Loading MNIST dataset...\")\n",
        "    batch_size = 100\n",
        "    train_loader, test_loader = load_mnist(batch_size)\n",
        "    print(\"Dataset loaded successfully!\")\n",
        "\n",
        "    # Define the network architecture\n",
        "    visible_size = 28 * 28   # MNIST image size\n",
        "    hidden_sizes = [500, 250, 100]  # 3 hidden layers\n",
        "    output_size = 10  # 10 digits\n",
        "\n",
        "    # Create the DBN classifier\n",
        "    print(\"\\nCreating DBN classifier...\")\n",
        "    dbn_classifier = DBNClassifier(visible_size, hidden_sizes, output_size)\n",
        "    print(\"DBN classifier created with architecture:\")\n",
        "    print(f\"Input layer: {visible_size} units\")\n",
        "    for i, size in enumerate(hidden_sizes):\n",
        "        print(f\"Hidden layer {i+1}: {size} units\")\n",
        "    print(f\"Output layer: {output_size} units\")\n",
        "\n",
        "    # Pretrain the DBN\n",
        "    print(\"\\nPretraining DBN layers...\")\n",
        "    dbn_classifier.pretrain(\n",
        "        train_loader,\n",
        "        epochs_per_layer=3,  # Use more epochs (10+) for better results\n",
        "        lr=0.01,\n",
        "        cd_k=1,\n",
        "        device=device\n",
        "    )\n",
        "    print(\"Pretraining completed!\")\n",
        "\n",
        "    # Visualize the weights of the first RBM layer\n",
        "    print(\"\\nVisualizing weights of the first RBM layer...\")\n",
        "    visualize_weights(dbn_classifier.dbn.rbm_layers[0], nrow=5, ncol=10, figsize=(10, 5))\n",
        "\n",
        "    # Generate samples from the first RBM layer\n",
        "    print(\"\\nGenerating samples from the first RBM layer...\")\n",
        "    samples = generate_samples(dbn_classifier.dbn.rbm_layers[0], num_samples=10, gibbs_steps=1000, device=device)\n",
        "\n",
        "    # Fine-tune the entire network\n",
        "    print(\"\\nFine-tuning the entire network...\")\n",
        "    history = dbn_classifier.finetune(\n",
        "        train_loader,\n",
        "        test_loader,\n",
        "        epochs=5,  # Use more epochs (10+) for better results\n",
        "        lr=0.001,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Plot training history\n",
        "    print(\"\\nPlotting training history...\")\n",
        "    plot_training_history(history)\n",
        "\n",
        "    # Evaluate the model on test data\n",
        "    print(\"\\nEvaluating model on test data...\")\n",
        "    dbn_classifier.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, targets in test_loader:\n",
        "            data = data.view(data.size(0), -1).to(device)\n",
        "            if data.max() > 1:\n",
        "                data = data / 255.0\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = dbn_classifier(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "    final_accuracy = 100 * correct / total\n",
        "    print(f\"Final test accuracy: {final_accuracy:.2f}%\")\n",
        "\n",
        "    # Save the model\n",
        "    print(\"\\nSaving model...\")\n",
        "    torch.save(dbn_classifier.state_dict(), 'dbn_mnist_model.pt')\n",
        "    print(\"Model saved successfully!\")\n",
        "\n",
        "    print(\"\\nDone!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "GwpdUpVqCMC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A1-xVgmuCL03"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}