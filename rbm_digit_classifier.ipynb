{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNpQck+EOf00L3iUgTaooP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thenaivekid/AI_Boltzmann_Machine/blob/main/rbm_digit_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeZ0SDCtj0Cq",
        "outputId": "2f595210-e201-433f-ee80-40e607499e01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-50' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 579, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 66, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 69, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 330, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n",
            "Exception ignored in: <coroutine object predict_digit at 0x78f076ca6130>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <lambda>\n",
            "KeyError: '__import__'\n",
            "Exception ignored in: <coroutine object predict_digit at 0x78f076ca6130>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <lambda>\n",
            "KeyError: '__import__'\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-42' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 579, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 66, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 69, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 330, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Load MNIST Dataset\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: (x > 0.5).float())])  # Binarize images\n",
        "# train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "# test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# RBM Class\n",
        "class RBM(nn.Module):\n",
        "    def __init__(self, num_visible, num_hidden):\n",
        "        super(RBM, self).__init__()\n",
        "        self.num_visible = num_visible\n",
        "        self.num_hidden = num_hidden\n",
        "        self.W = nn.Parameter(torch.randn(num_hidden, num_visible) * 0.01)  # Weights\n",
        "        self.v_bias = nn.Parameter(torch.zeros(num_visible))  # Visible bias\n",
        "        self.h_bias = nn.Parameter(torch.zeros(num_hidden))  # Hidden bias\n",
        "\n",
        "    def forward(self, v):\n",
        "        \"\"\"One Gibbs sampling step: v -> h -> v'\"\"\"\n",
        "        h_prob = torch.sigmoid(torch.matmul(v, self.W.T) + self.h_bias)  # P(h|v)\n",
        "        h_state = (torch.rand_like(h_prob) < h_prob).float()  # Sample h\n",
        "        v_prob = torch.sigmoid(torch.matmul(h_state, self.W) + self.v_bias)  # P(v|h)\n",
        "        v_state = (torch.rand_like(v_prob) < v_prob).float()  # Sample v\n",
        "        return v_prob, v_state\n",
        "\n",
        "    def free_energy(self, v):\n",
        "        \"\"\"Energy function for Contrastive Divergence.\"\"\"\n",
        "        # Term 1: Visible bias term (v^T * b_v)\n",
        "        vb_term = torch.matmul(v, self.v_bias)\n",
        "\n",
        "        # Term 2: Hidden term (sum over log(1 + exp(v^T * W_j + b_h_j)))\n",
        "        hidden_term = torch.sum(\n",
        "            torch.log(1 + torch.exp(torch.matmul(v, self.W.T) + self.h_bias)),\n",
        "            dim=1\n",
        "        )\n",
        "\n",
        "        # Free energy: F(v) = -vb_term - hidden_term\n",
        "        return -vb_term - hidden_term\n",
        "\n",
        "    def train_rbm(self, train_loader, lr=0.001, epochs=50):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=lr)  # Use Adam instead of SGD\n",
        "        loss_history = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0\n",
        "            for batch, (data, _) in enumerate(train_loader):\n",
        "                v0 = data.view(-1, 28*28).to(device)  # Flatten images & move to GPU\n",
        "                v1_prob, v1_state = self.forward(v0)  # Gibbs sampling\n",
        "\n",
        "                # Compute gradients using Contrastive Divergence (CD-1)\n",
        "                loss = torch.mean(self.free_energy(v0)) - torch.mean(self.free_energy(v1_state))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            avg_loss = epoch_loss / len(train_loader)\n",
        "            loss_history.append(avg_loss)\n",
        "            print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "            # Visualize Weights & Reconstructions\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                self.visualize_weights()\n",
        "                self.visualize_reconstruction(v0, v1_prob)\n",
        "\n",
        "        # Plot loss curve\n",
        "        self.plot_loss(loss_history)\n",
        "\n",
        "    def plot_loss(self, loss_history):\n",
        "        \"\"\"Plot training loss curve.\"\"\"\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.plot(loss_history, label=\"Loss\")\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"Free Energy Loss\")\n",
        "        plt.title(\"RBM Training Loss Curve\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    def visualize_weights(self, num_images=16):\n",
        "        \"\"\"Plot learned features (weights).\"\"\"\n",
        "        fig, axes = plt.subplots(4, 4, figsize=(6, 6))\n",
        "        for i, ax in enumerate(axes.flatten()):\n",
        "            if i >= num_images:\n",
        "                break\n",
        "            weight_img = self.W[i].detach().cpu().view(28, 28)\n",
        "            ax.imshow(weight_img, cmap=\"gray\")\n",
        "            ax.axis(\"off\")\n",
        "        plt.suptitle(\"RBM Learned Features\")\n",
        "        plt.show()\n",
        "\n",
        "    def visualize_reconstruction(self, original, reconstructed, num_images=10):\n",
        "        \"\"\"Visualize original and reconstructed images.\"\"\"\n",
        "        fig, axes = plt.subplots(2, num_images, figsize=(15, 3))\n",
        "        for i in range(num_images):\n",
        "            # Original\n",
        "            axes[0, i].imshow(original[i].detach().cpu().view(28, 28), cmap=\"gray\")\n",
        "            axes[0, i].axis(\"off\")\n",
        "\n",
        "            # Reconstructed\n",
        "            axes[1, i].imshow(reconstructed[i].detach().cpu().view(28, 28), cmap=\"gray\")\n",
        "            axes[1, i].axis(\"off\")\n",
        "\n",
        "        axes[0, 0].set_title(\"Original Images\")\n",
        "        axes[1, 0].set_title(\"Reconstructed Images\")\n",
        "        plt.show()\n",
        "\n",
        "    def evaluate(self, test_loader):\n",
        "        \"\"\"Evaluate RBM on unseen test data using MSE, SSIM, and PSNR.\"\"\"\n",
        "        mse_total, ssim_total, psnr_total, count = 0, 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for batch, (data, _) in enumerate(test_loader):\n",
        "                v0 = data.view(-1, 28*28).to(device)  # Flatten images & move to GPU\n",
        "                v1_prob, _ = self.forward(v0)  # Reconstruct images\n",
        "\n",
        "                # Convert to numpy for metric calculations\n",
        "                original_np = v0.cpu().numpy()\n",
        "                reconstructed_np = v1_prob.cpu().numpy()\n",
        "\n",
        "                # Compute MSE, SSIM, PSNR\n",
        "                for i in range(original_np.shape[0]):\n",
        "                    mse = np.mean((original_np[i] - reconstructed_np[i])**2)\n",
        "                    ssim_score = ssim(original_np[i].reshape(28, 28), reconstructed_np[i].reshape(28, 28), data_range=1)\n",
        "                    psnr_score = psnr(original_np[i], reconstructed_np[i], data_range=1)\n",
        "\n",
        "                    mse_total += mse\n",
        "                    ssim_total += ssim_score\n",
        "                    psnr_total += psnr_score\n",
        "                    count += 1\n",
        "\n",
        "\n",
        "\n",
        "        # Print results\n",
        "        print(\"\\n--- RBM Evaluation on Unseen Data ---\")\n",
        "        print(f\"Mean Squared Error (MSE): {mse_total / count:.5f}\")\n",
        "        print(f\"Structural Similarity Index (SSIM): {ssim_total / count:.5f}\")\n",
        "        print(f\"Peak Signal-to-Noise Ratio (PSNR): {psnr_total / count:.5f}\")\n",
        "\n",
        "        # Visualize some reconstructions\n",
        "        self.visualize_reconstruction(v0, v1_prob)\n",
        "\n",
        "    def extract_features(self, data_loader):\n",
        "        \"\"\"Extract hidden layer features for classification\"\"\"\n",
        "        features, labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch, (data, targets) in enumerate(data_loader):\n",
        "                v = data.view(-1, 28 * 28).to(device)\n",
        "                h_prob, _ = self.forward(v)  # Get hidden layer activations\n",
        "                features.append(h_prob.cpu().numpy())\n",
        "                labels.append(targets.cpu().numpy())\n",
        "\n",
        "        return np.vstack(features), np.hstack(labels)\n",
        "\n",
        "# # Train RBM\n",
        "# rbm = RBM(num_visible=28*28, num_hidden=256).to(device)  # 256 hidden neurons, move to GPU\n",
        "# rbm.train_rbm(train_loader, lr=0.001, epochs=50)\n",
        "\n",
        "# # Evaluate on Unseen Test Data\n",
        "# rbm.evaluate(test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# torch.save(rbm.state_dict(), \"/content/drive/MyDrive/research/rbm_mnist.pth\")\n",
        "# print(\"✅ Model saved as rbm_mnist.pth\")\n",
        "\n",
        "# Load the saved model\n",
        "# rbm_loaded = RBM(num_visible=28*28, num_hidden=256).to(device)  # Initialize same architecture\n",
        "# rbm_loaded.load_state_dict(torch.load(\"/content/drive/MyDrive/research/rbm_mnist.pth\", map_location=torch.device('cpu')))\n",
        "# rbm_loaded.eval()  # Set to evaluation mode\n",
        "# print(\"✅ Model loaded successfully\")\n",
        "# rbm = rbm_loaded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewQuENtZj5z0",
        "outputId": "ea27cf97-ec58-4d9c-c87e-8d01fcc478cb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import torchvision\n",
        "# import torchvision.transforms as transforms\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# from sklearn.svm import SVC\n",
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "# # Extract features using the trained RBM\n",
        "# train_features, train_labels = rbm.extract_features(train_loader)\n",
        "# test_features, test_labels = rbm.extract_features(test_loader)\n",
        "\n",
        "# # Train SVM Classifier\n",
        "# classifier = SVC(kernel='rbf', C=1.0, gamma='scale')  # RBF kernel for better performance\n",
        "# classifier.fit(train_features, train_labels)\n",
        "\n",
        "# # Evaluate Model\n",
        "# predictions = classifier.predict(test_features)\n",
        "# accuracy = accuracy_score(test_labels, predictions)\n",
        "# print(f\"Classification Accuracy: {accuracy:.4f}\")\n",
        "# import joblib\n",
        "# joblib.dump(classifier, \"/content/drive/MyDrive/research/logistic_classifier.pkl\")\n",
        "\n",
        "# print(\"Models saved successfully!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ElGu6Uonj7vx"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import joblib\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Load the trained RBM and SVM model\n",
        "device = torch.device(\"cpu\")\n",
        "rbm_path = \"/content/drive/MyDrive/research/rbm_mnist.pth\"\n",
        "rbm = RBM(num_visible=28*28, num_hidden=256).to(device)  # Initialize same architecture\n",
        "rbm.load_state_dict(torch.load(rbm_path, map_location=device))\n",
        "rbm.eval()  # Set model to evaluation mode\n",
        "\n",
        "# Load trained SVM classifier\n",
        "svm_path = \"/content/drive/MyDrive/research/svm_classifier.pkl\"\n",
        "svm_classifier = joblib.load(svm_path)\n",
        "\n",
        "# Define image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),  # Resize to MNIST size\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "def predict_digit(image_path: str,rbm=rbm, svm_classifier=svm_classifier, transform=transform):\n",
        "    \"\"\"\n",
        "    Predicts the digit in the image using the trained RBM and SVM classifier.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the input image.\n",
        "\n",
        "    Returns:\n",
        "        int: Predicted digit label.\n",
        "    \"\"\"\n",
        "    # Load and preprocess the image\n",
        "    image = Image.open(image_path).convert(\"L\")  # Convert to grayscale\n",
        "    image = transform(image)  # Apply preprocessing\n",
        "    image = image.view(-1, 28*28)  # Flatten image\n",
        "\n",
        "    # Extract features using the trained RBM\n",
        "    with torch.no_grad():\n",
        "        features,_ = rbm(image)  # Extract RBM features\n",
        "\n",
        "    # Convert features to numpy array for SVM input\n",
        "    # features = features.numpy().reshape(1, -1)  # Flatten the feature vector\n",
        "\n",
        "    # Predict with SVM classifier\n",
        "\n",
        "    predicted_label = svm_classifier.predict(features)[0]\n",
        "\n",
        "    return int(predicted_label)\n",
        "\n",
        "# # Example usage\n",
        "image_path = \"image.jpg\"  # Path to an image file\n",
        "predicted_digit = predict_digit(image_path)\n",
        "print(f\"Predicted Digit: {predicted_digit}\")\n",
        "type((predicted_digit))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rb-14po7kU_t",
        "outputId": "5a523e99-e45d-4821-b0b6-dfff4cad7c2f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Digit: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-b70137a44238>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  rbm.load_state_dict(torch.load(rbm_path, map_location=device))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "int"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q fastapi uvicorn python-multipart torch torchvision pytorchvideo pyngrok\n"
      ],
      "metadata": {
        "id": "qEh_zG5WkYR1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uvicorn\n",
        "import torch\n",
        "import joblib\n",
        "import numpy as np\n",
        "from fastapi import FastAPI, File, UploadFile\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "\n",
        "# Initialize FastAPI\n",
        "app = FastAPI()\n",
        "\n",
        "# Allow all CORS requests\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "\n",
        "@app.post(\"/predict/\")\n",
        "async def predict_digit_from_image(file: UploadFile = File(...)):\n",
        "    image_path = \"image.jpg\"\n",
        "\n",
        "    # Write the uploaded file content to the image path\n",
        "    with open(image_path, \"wb\") as f:\n",
        "        f.write(await file.read())\n",
        "\n",
        "\n",
        "    print(image_path)\n",
        "    # Pass the file path to the predict_digit function\n",
        "    predicted_label = predict_digit(image_path)\n",
        "    print(f\"predicted {predicted_label}, {type(predicted_label)}\")\n",
        "    # Clean up the temp file after prediction\n",
        "\n",
        "    return {\"predicted_digit\": predicted_label}\n",
        "if __name__ == \"__main__\":\n",
        "    import os\n",
        "    os.environ[\"NGROK_AUTHTOKEN\"] = \"2rCbW45ffaTmVf03HbMluTUNCv1_4uD242hh56wg9SvHorrNR\"\n",
        "\n",
        "    ngrok_tunnel = ngrok.connect(8000)\n",
        "    print('Public_URL:', ngrok_tunnel.public_url)\n",
        "\n",
        "    nest_asyncio.apply()\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YB-Aw6RckZXu",
        "outputId": "1762377d-fcb2-47db-a87e-41858c602fe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public_URL: https://79ce-35-185-110-153.ngrok-free.app\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [208]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     2407:5200:405:37a2:9028:1cb0:7248:e92d:0 - \"GET / HTTP/1.1\" 404 Not Found\n",
            "INFO:     2407:5200:405:37a2:9028:1cb0:7248:e92d:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "INFO:     2407:5200:405:37a2:9028:1cb0:7248:e92d:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     2407:5200:405:37a2:9028:1cb0:7248:e92d:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            "image.jpg\n",
            "predicted 3, <class 'int'>\n",
            "INFO:     2407:5200:405:37a2:9028:1cb0:7248:e92d:0 - \"POST /predict/ HTTP/1.1\" 200 OK\n"
          ]
        }
      ]
    }
  ]
}